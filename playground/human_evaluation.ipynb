{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 17:36:54.182722: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataset import get_train_test_data, get_data_masks\n",
    "\n",
    "# Read the featuremaps clusters data\n",
    "featuremaps_df = pd.read_pickle('../logs/feature_combinations_clusters')\n",
    "# Compute the total time\n",
    "featuremaps_df['time'] = featuremaps_df['map_time'] + featuremaps_df['features_extraction']\n",
    "featuremaps_df = featuremaps_df.drop(columns=['map_time', 'features_extraction'])\n",
    "\n",
    "# Filter for the desired approach\n",
    "featuremaps_df = featuremaps_df[\n",
    "    # (featuremaps_df['map_size']  == '10x10') &\n",
    "    (featuremaps_df['mode'] == 'original')\n",
    "]\n",
    "\n",
    "featuremaps_df['approach'] = featuremaps_df.apply(\n",
    "    lambda row: f'{row[\"approach\"]}({row[\"map_size\"]})_{row[\"mode\"]}',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Read the heatmaps data\n",
    "heatmaps_df = pd.read_pickle('../logs/heatmaps_data')\n",
    "\n",
    "# Drop the values with null silhouette score\n",
    "heatmaps_df = heatmaps_df.dropna(subset=['silhouette']).reset_index(drop=True)\n",
    "# Keep the column of interest\n",
    "heatmaps_df = heatmaps_df[['clustering_mode', 'explainer', 'clustering_technique', 'clusters', 'time_clustering', 'time_contributions']]\n",
    "heatmaps_df = heatmaps_df.rename(columns={'explainer': 'approach'})\n",
    "# Merge the information for the clustering mode\n",
    "heatmaps_df['clustering_mode'] = heatmaps_df.apply(lambda row: f'{row[\"clustering_technique\"]}({row[\"clustering_mode\"]})', axis=1)\n",
    "heatmaps_df = heatmaps_df.drop(columns=['clustering_technique'])\n",
    "# Compute the total time\n",
    "heatmaps_df['time'] = heatmaps_df['time_clustering'] + heatmaps_df['time_contributions']\n",
    "heatmaps_df = heatmaps_df.drop(columns=['time_clustering', 'time_contributions'])\n",
    "\n",
    "heatmaps_df.head()\n",
    "\n",
    "# Merge all the clusters together\n",
    "complete_df = pd.concat([featuremaps_df, heatmaps_df]).reset_index(drop=True)\n",
    "\n",
    "# Extract the data about the number of clusters\n",
    "complete_df['num_clusters'] = complete_df['clusters'].apply(len)\n",
    "# Extract data about the clusters sizes\n",
    "complete_df['clusters_sizes'] = complete_df['clusters'].apply(lambda clusters: [len(cluster) for cluster in clusters])\n",
    "\n",
    "# Get the indexes of the misclassified elements\n",
    "(train_data, train_labels), (test_data, test_labels) = get_train_test_data(rgb=True)\n",
    "predictions = np.loadtxt('../in/predictions.csv')\n",
    "mask_miss, mask_label = get_data_masks(test_labels, predictions, label=5)\n",
    "mask_miss_label = mask_miss[mask_label]\n",
    "misclassified_idxs = np.argwhere(mask_miss_label == True)\n",
    "# Find the fraction of misclassified data in each cluster\n",
    "complete_df['frac_misses'] = complete_df['clusters'].apply(\n",
    "    lambda clusters: [\n",
    "        len([entry for entry in cluster if entry in misclassified_idxs]) / len(cluster)\n",
    "        for cluster in clusters\n",
    "    ]\n",
    ")\n",
    "# Find the fraction of clusters containing both correct and incorrect classifications\n",
    "complete_df['frac_mixed'] = complete_df['frac_misses'].apply(lambda misses: len([entry for entry in misses if 0 < entry < 1]) / len(misses))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([list([513, 516, 133, 138, 525, 526, 528, 657, 658, 148, 532, 790, 23, 538, 669, 160, 681, 433, 51, 53, 54, 438, 568, 61, 445, 701, 834, 207, 209, 86, 598, 729, 608, 482, 231, 487, 620, 114, 115, 761, 120, 377, 378, 764]),\n        list([769, 130, 262, 519, 137, 394, 395, 396, 523, 19, 147, 24, 156, 540, 286, 541, 542, 162, 164, 37, 548, 167, 168, 41, 297, 550, 551, 46, 175, 305, 562, 691, 188, 191, 66, 68, 198, 455, 78, 206, 81, 212, 217, 103, 362, 364, 237, 118, 247, 505, 125]),\n        list([772, 263, 520, 266, 524, 13, 270, 527, 531, 404, 534, 415, 40, 169, 302, 690, 311, 312, 567, 314, 697, 67, 200, 210, 600, 480, 739, 100, 488, 621, 110, 238, 239, 368, 495, 622, 372, 373, 374, 119, 633, 634, 251, 763, 127])],\n       dtype=object),\n array([list([384, 132, 264, 648, 522, 271, 401, 402, 403, 530, 22, 407, 535, 539, 413, 417, 807, 43, 439, 184, 186, 319, 577, 196, 709, 73, 844, 335, 213, 344, 89, 218, 728, 223, 864, 610, 358, 870, 233, 873, 236, 880, 116, 117, 885, 383]),\n        list([386, 655, 783, 17, 32, 549, 677, 552, 553, 812, 301, 813, 559, 309, 310, 58, 703, 195, 324, 710, 841, 714, 715, 843, 464, 83, 725, 726, 856, 859, 108, 623, 887])],\n       dtype=object))"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from utils.general import get_balanced_samples\n",
    "from utils.cluster.postprocessing import get_misses_count, get_labels_purity\n",
    "\n",
    "# Get the clusters for the selected approach\n",
    "clusters = np.array(complete_df.set_index('approach').loc['GradCAM'].iloc[0]['clusters'], dtype=list)\n",
    "# Find the count of misclassified entries in each cluster\n",
    "counts_misses = np.vectorize(lambda cluster: get_misses_count(cluster, predictions=predictions))(clusters)\n",
    "# Find the purity and impurity of each cluster\n",
    "purities = np.vectorize(lambda cluster: get_labels_purity(cluster, predictions=predictions)) (clusters)\n",
    "# Weight the purity and impurity based on the count of misclassified elements in log scale\n",
    "counts_misses_log = np.vectorize(lambda val: 0 if val == 0 else np.log(val))(counts_misses)\n",
    "\n",
    "pure_sample, impure_sample = get_balanced_samples(clusters, sample_size=5, balanced_by=purities, weights=counts_misses_log)\n",
    "\n",
    "pure_sample, impure_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}